{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Constraints from Dataset Annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how constraints can be extracted from dataset annotation\n",
    "and compiled into formulas.\n",
    "\n",
    "For both 50Salads and Breakfast dataset, we will:\n",
    "\n",
    "1. Collect statistics.\n",
    "2. Compiled constraint strings.\n",
    "3. Verify constraint strings against annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "from glob import glob\n",
    "import json\n",
    "import os.path as osp\n",
    "import pickle as pk\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "from lib.tl import parser\n",
    "\n",
    "# Raise the recursion limit to avoid problems when parsing formulas\n",
    "sys.setrecursionlimit(10000)\n",
    "# Setting TL_RECORD_TRACE asks DTL to record the evaluation trace.\n",
    "# Using this we can find the conflicting part between logits and formula.\n",
    "os.environ['TL_RECORD_TRACE'] = '1'\n",
    "data_root = '../dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four function below collects backward-dependency (BD), \n",
    "forward-cancellation (FC), implication (Ip) and exclusivity (Ex).\n",
    "\n",
    "For all the functions:\n",
    "\n",
    "- The first return value is a list of stringified logic clauses (constraints).\n",
    "- The second return value is a list of tuples of ordered operands. \n",
    "  - For example, a tuple `(x,y)` in the list returned by `bd_clauses` means\n",
    "        `BD(x, y)`, or action x is back dependent on action y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_dependency_clauses(SUBCLASSES, occur_joint, occur_before, compact=False):\n",
    "    clauses = []\n",
    "    ordered_operands = []\n",
    "    for i in range(0, len(SUBCLASSES)):\n",
    "        for j in range(0, len(SUBCLASSES)):\n",
    "            if i == j: continue\n",
    "            if occur_joint[i,j] > 0 and occur_before[i,j] == 0:\n",
    "                act_i, act_j = SUBCLASSES[i].lower(), SUBCLASSES[j].lower()\n",
    "                clauses.append(f'((F {act_i} & F {act_j}) -> (~{act_i} W {act_j}))')\n",
    "                ordered_operands.append((act_i, act_j))\n",
    "    return clauses, ordered_operands\n",
    "\n",
    "\n",
    "def mutual_exclusivity_clauses(SUBCLASSES, occur_joint, compact=False):\n",
    "    clauses = []\n",
    "    ordered_operands = []\n",
    "    for i in range(0, len(SUBCLASSES)):\n",
    "        act_i = SUBCLASSES[i].lower()\n",
    "        if compact: sub_clauses_i = []\n",
    "        for j in range(0, len(SUBCLASSES)):\n",
    "            act_j = SUBCLASSES[j].lower()\n",
    "            if i == j: continue\n",
    "            if occur_joint[i,j] == 0:\n",
    "                if not compact:\n",
    "                    clauses.append(f'(F {act_i} -> ~F {act_j})')\n",
    "                    ordered_operands.append((act_i, act_j))\n",
    "                else:\n",
    "                    sub_clauses_i.append(f'~F {act_j}')\n",
    "        if compact and len(sub_clauses_i) > 0:\n",
    "            clauses.append(f'(F {act_i} -> ({\" & \".join(sub_clauses_i)}))')\n",
    "    return clauses, ordered_operands\n",
    "\n",
    "\n",
    "def forward_cancellation_clauses(SUBCLASSES, occur_joint, occur_after, compact=False):\n",
    "    clauses = []\n",
    "    ordered_operands = []\n",
    "    for i in range(0, len(SUBCLASSES)):\n",
    "        for j in range(0, len(SUBCLASSES)):\n",
    "            if occur_joint[i,j] > 0 and occur_after[i,j] == 0:\n",
    "                act_i, act_j = SUBCLASSES[i].lower(), SUBCLASSES[j].lower()\n",
    "                if act_i != act_j:\n",
    "                    clauses.append(f'((F {act_i} & F {act_j}) -> (~{act_i} S {act_j}))')\n",
    "                    ordered_operands.append((act_j, act_i))\n",
    "                else:\n",
    "                    clauses.append(f'(F {act_i} -> (~{act_i} S {act_j}))')\n",
    "    return clauses, ordered_operands\n",
    "\n",
    "\n",
    "def implication_clauses(SUBCLASSES, occur_joint_vid, occur_count_vid, compact=False):\n",
    "    clauses = []\n",
    "    ordered_operands = []\n",
    "    for i in range(0, len(SUBCLASSES)):\n",
    "        act_i = SUBCLASSES[i].lower()\n",
    "        occur_joint_ij = occur_joint_vid[i] / occur_count_vid\n",
    "        if compact: sub_clauses_i = []\n",
    "        for j in range(0, len(SUBCLASSES)):\n",
    "            act_j = SUBCLASSES[j].lower()\n",
    "            if i == j: continue\n",
    "            if occur_joint_ij[j] >= 1:\n",
    "                if not compact:\n",
    "                    clauses.append(f'(F {act_j} -> F {act_i})')\n",
    "                    ordered_operands.append((act_j, act_i))\n",
    "                else:\n",
    "                    sub_clauses_i.append(f'F {act_j}')\n",
    "        if compact and len(sub_clauses_i) > 0:\n",
    "            clauses.append(f'(({\" | \".join(sub_clauses_i)}) -> F {act_i})')\n",
    "    return clauses, ordered_operands\n",
    "\n",
    "\n",
    "def read_file(path, gt_mode):\n",
    "    with open(path, 'r') as f:\n",
    "        gt = f.readlines()\n",
    "    if gt_mode:\n",
    "        gt = np.array([g[:-1] for g in gt])\n",
    "    else:\n",
    "        gt = np.array(gt[1].split())\n",
    "    return gt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule from Annotation (Frequentist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 Salad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBCLASSES = [\n",
    "    'cut_tomato', 'place_tomato_into_bowl', 'cut_cheese', \n",
    "    'place_cheese_into_bowl', 'cut_lettuce', 'place_lettuce_into_bowl', \n",
    "    'add_salt', 'add_vinegar', 'add_oil', 'add_pepper', 'mix_dressing', \n",
    "    'peel_cucumber', 'cut_cucumber', 'place_cucumber_into_bowl', \n",
    "    'add_dressing', 'mix_ingredients', 'serve_salad_onto_plate', \n",
    "    'action_start', 'action_end']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split can be set to 0, which means all samples will be used; \n",
    "# or 1~5, which means using the training samples of the corresponding split.\n",
    "split = 1\n",
    "with open(f'/home/ziwei/projects/compositional_action/dataset/50salads/splits/train.split{split}.bundle', 'r') as f:\n",
    "    fns = f.read().splitlines()\n",
    "\n",
    "annotation_files = ['../dataset/50salads/groundTruth/' + i for i in fns]\n",
    "\n",
    "occur_before = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_after = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_joint = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_joint_vid = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_count = np.zeros((len(SUBCLASSES,)))\n",
    "occur_count_vid = np.zeros((len(SUBCLASSES,)))\n",
    "\n",
    "anno_ = []\n",
    "for anno in tqdm(annotation_files):\n",
    "    anno_items = open(anno).read().splitlines()\n",
    "    actions = [anno_items[0]]\n",
    "    for i in range(1, len(anno_items)):\n",
    "        if anno_items[i] != anno_items[i-1]: actions.append(anno_items[i])\n",
    "    \n",
    "    occur_before_record = []\n",
    "    occur_after_record = []\n",
    "    occur_joint_record = []\n",
    "    occur_record = []\n",
    "\n",
    "    for i, act in enumerate(actions):\n",
    "        an = act\n",
    "        act_i = SUBCLASSES.index(act)\n",
    "\n",
    "        occur_count[act_i] += 1\n",
    "        if act not in occur_record:\n",
    "            occur_count_vid[act_i] += 1\n",
    "            occur_record.append(act)\n",
    "\n",
    "        for j in range(0, i):\n",
    "            act_j = SUBCLASSES.index(actions[j])\n",
    "            if (act_j, act_i) not in occur_joint_record:\n",
    "                occur_joint_vid[act_j, act_i] += 1\n",
    "                occur_joint_record.append((act_j, act_i))\n",
    "            occur_before[act_j, act_i] += 1\n",
    "            occur_joint[act_j, act_i] += 1\n",
    "\n",
    "        for j in range(i+1, len(actions)):\n",
    "            act_j = SUBCLASSES.index(actions[j])\n",
    "            if (act_j, act_i) not in occur_joint_record:\n",
    "                occur_joint_vid[act_j, act_i] += 1\n",
    "                occur_joint_record.append((act_j, act_i))\n",
    "            occur_after[act_j, act_i] += 1\n",
    "            occur_joint[act_j, act_i] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate rule strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BD: ~a_1 W a_2\n",
    "bd_str = backward_dependency_clauses(SUBCLASSES, occur_joint, occur_before)[0]\n",
    "# Ex: F a_1 -> ~F a_2 \n",
    "ex_str = mutual_exclusivity_clauses(SUBCLASSES, occur_joint)[0]\n",
    "# FC: ~a_1 S a_2\n",
    "fc_str = forward_cancellation_clauses(SUBCLASSES, occur_joint, occur_after)[0]\n",
    "# Ip: F a_1 -> F a_2\n",
    "ip_str = implication_clauses(SUBCLASSES, occur_joint_vid, occur_count_vid)[0]\n",
    "# The combined rule string\n",
    "rule_str = '(' + ' & '.join([c for k in [bd_str, ex_str, fc_str, ip_str] for c in k if len(k) > 0]) + ')'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify constraints.\n",
    "Note that if you set `split` to non-zero values, the constraints will be wrong for some samples.\n",
    "This is because the constraints are formed from a partial observation of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = parser.parse(rule_str)\n",
    "\n",
    "ap_map = lambda x: SUBCLASSES.index(x)\n",
    "annotation_files = glob(osp.join(data_root, '50salads', 'groundTruth', '*.txt'))\n",
    "for fn in tqdm(annotation_files):\n",
    "    gt_label = read_file(fn, True)\n",
    "\n",
    "    gt_array = -1 * torch.ones((len(SUBCLASSES), len(gt_label)))\n",
    "    for t, l in enumerate(gt_label):\n",
    "        gt_array[SUBCLASSES.index(l), t] = 1\n",
    "    tl_value = evaluator(gt_array, ap_map=lambda x: SUBCLASSES.index(x), rho=1000)\n",
    "\n",
    "    if tl_value < 0.0: \n",
    "        print('conflict found: ', fn)        \n",
    "        for i, children in enumerate(evaluator.children):\n",
    "            if children.value.min() < 0.0:\n",
    "                print('clause', i, children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakfast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule file generation (SIL ignored)\n",
    "SUBCLASSES = ['take_cup', 'pour_coffee', 'pour_milk', 'pour_sugar', \n",
    "    'stir_coffee', 'spoon_sugar', 'add_teabag', 'pour_water', 'stir_tea', \n",
    "    'cut_bun', 'smear_butter', 'put_toppingOnTop', 'put_bunTogether', \n",
    "    'take_plate', 'take_knife', 'take_butter', 'take_topping', 'cut_orange', \n",
    "    'squeeze_orange', 'take_glass', 'pour_juice', 'take_squeezer', 'take_bowl', \n",
    "    'pour_cereals', 'stir_cereals', 'spoon_powder', 'stir_milk', 'pour_oil', \n",
    "    'take_eggs', 'crack_egg', 'add_saltnpepper', 'fry_egg', 'put_egg2plate', \n",
    "    'butter_pan', 'cut_fruit', 'put_fruit2bowl', 'peel_fruit', 'stir_fruit', \n",
    "    'stirfry_egg', 'stir_egg', 'pour_egg2pan', 'spoon_flour', 'stir_dough', \n",
    "    'pour_dough2pan', 'fry_pancake', 'put_pancake2plate', 'pour_flour', 'SIL']\n",
    "\n",
    "METACLASSES = [\n",
    "    'coffee', 'tea', 'sandwich', 'juice', 'cereals', \n",
    "    'milk', 'friedegg', 'salat', 'scrambledegg', 'pancake'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split can be set to 0, which means all samples will be used; \n",
    "# or 1~4, which means using the training samples of the corresponding split.\n",
    "split = 0\n",
    "with open(f'{data_root}/breakfast/splits/train.split{split}.bundle', 'r') as f:\n",
    "    fns = f.read().splitlines()\n",
    "fns = [f'{data_root}/breakfast/groundTruth/' + i for i in fns]\n",
    "\n",
    "annotation_files = fns\n",
    "\n",
    "occur_before = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_after = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_joint = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_joint_vid = np.zeros((len(SUBCLASSES), len(SUBCLASSES)))\n",
    "occur_count = np.zeros((len(SUBCLASSES,)))\n",
    "occur_count_vid = np.zeros((len(SUBCLASSES,)))\n",
    "\n",
    "for anno in tqdm(annotation_files):\n",
    "    with open(anno, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l[:-1] for l in lines]\n",
    "\n",
    "    meta_act = anno.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "\n",
    "    actions = [lines[0]]\n",
    "    for l in lines[1:]:\n",
    "        if l != actions[-1]:\n",
    "            actions.append(l)\n",
    "\n",
    "    occur_before_record = []\n",
    "    occur_after_record = []\n",
    "    occur_joint_record = []\n",
    "    occur_record = []\n",
    "\n",
    "    for i, act in enumerate(actions):\n",
    "        act_i = SUBCLASSES.index(act)\n",
    "\n",
    "        occur_count[act_i] += 1\n",
    "        if act not in occur_record:\n",
    "            occur_count_vid[act_i] += 1\n",
    "            occur_record.append(act)\n",
    "\n",
    "        for j in range(0, i):\n",
    "            act_j = SUBCLASSES.index(actions[j])\n",
    "            if (act_j, act_i) not in occur_joint_record:\n",
    "                occur_joint_vid[(act_j, act_i)] += 1\n",
    "                occur_joint_record.append((act_j, act_i))\n",
    "            occur_before[act_j, act_i] += 1\n",
    "            occur_joint[act_j, act_i] += 1\n",
    "\n",
    "        for j in range(i+1, len(actions)):\n",
    "            act_j = SUBCLASSES.index(actions[j])\n",
    "            if (act_j, act_i) not in occur_joint_record:\n",
    "                occur_joint_vid[(act_j, act_i)] += 1\n",
    "                occur_joint_record.append((act_j, act_i))\n",
    "            occur_after[act_j, act_i] += 1\n",
    "            occur_joint[act_j, act_i] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate rule strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BD: ~a_1 W a_2\n",
    "bd_str = backward_dependency_clauses(SUBCLASSES, occur_joint, occur_before)[0]\n",
    "# Ex: F a_1 -> ~F a_2 \n",
    "ex_str = mutual_exclusivity_clauses(SUBCLASSES, occur_joint)[0]\n",
    "# FC: ~a_1 S a_2\n",
    "fc_str = forward_cancellation_clauses(SUBCLASSES, occur_joint, occur_after)[0]\n",
    "# Ip: F a_1 -> F a_2\n",
    "ip_str = implication_clauses(SUBCLASSES, occur_joint_vid, occur_count_vid)[0]\n",
    "# The combined rule string\n",
    "rule_str = '(' + ' & '.join([c for k in [bd_str, ex_str, fc_str, ip_str] for c in k if len(k) > 0]) + ')'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify constraints.\n",
    "This can be much slower than 50Salads given the number of constraints.\n",
    "\n",
    "Note that if you set `split` to non-zero values, the constraints will be wrong for some samples.\n",
    "This is because the constraints are formed from a partial observation of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_files = glob(osp.join(data_root, 'breakfast', 'groundTruth', '*.txt'))\n",
    "SUBCLASSES_ = [i.lower() for i in SUBCLASSES]\n",
    "ap_map = lambda x: SUBCLASSES_.index(x)\n",
    "\n",
    "evaluator = parser.parse(rule_str)\n",
    "\n",
    "for fn in tqdm(annotation_files):\n",
    "    gt_label = read_file(fn, True)\n",
    "\n",
    "    gt_array = -1 * torch.ones((len(SUBCLASSES), len(gt_label)))\n",
    "    for t, l in enumerate(gt_label):\n",
    "        gt_array[SUBCLASSES_.index(l.lower()), t] = 1\n",
    "    tl_value = evaluator(gt_array, ap_map=lambda x: SUBCLASSES_.index(x), rho=1000)\n",
    "\n",
    "    if tl_value < 0.0: \n",
    "        print('conflict found: ', fn)        \n",
    "        for i, children in enumerate(evaluator.children):\n",
    "            if children.value.min() < 0.0:\n",
    "                print('clause', i, children)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c459a13c392e2b53f327265b88bd0b184bbc8ffb8c351d8a3286a23767f1eed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
